#!/usr/bin/env python3
"""
Interactive Ultra Context Analyzer Tester
Ultra-fine-tuned with enhanced precision!
"""

from ultra_context_analyzer import UltraContextAnalyzer

def interactive_ultra_test():
    """Interactive testing interface for the Ultra Context Analyzer."""
    
    print("üöÄ Ultra-Fine-Tuned Context Analyzer v2.0 - Interactive Tester")
    print("=" * 75)
    print("‚ú® Enhanced with: Smart disambiguation, absurdity detection, quality scoring")
    print("üéØ Test with any prompt to see ultra-precise context understanding!")
    print("Type 'quit' or 'exit' to stop")
    print("=" * 75)
    
    analyzer = UltraContextAnalyzer()
    
    while True:
        print("\nüîç Enter your prompt:")
        prompt = input("‚û§ ").strip()
        
        if prompt.lower() in ['quit', 'exit', 'q']:
            print("\nüëã Thanks for testing the Ultra Analyzer! Goodbye!")
            break
        
        if not prompt:
            print("‚ùå Please enter a prompt!")
            continue
        
        print(f"\nüéØ Ultra-Analyzing: '{prompt}'")
        print("-" * 60)
        
        try:
            # Get comprehensive ultra analysis
            context = analyzer.analyze_ultra_context(prompt)
            overall = context['overall_context']
            
            # Get ultra model selection
            model, confidence, reasoning = analyzer.select_ultra_model(prompt)
            
            # Display enhanced results
            print(f"üéØ Primary Intent: {overall['primary_intent'].replace('_', ' ').title()}")
            if context['intent_analysis']['secondary_intent']:
                print(f"üîÑ Secondary Intent: {context['intent_analysis']['secondary_intent'].replace('_', ' ').title()}")
            
            print(f"üè∑Ô∏è  Primary Domain: {overall['primary_domain'].title()}")
            active_domains = context['domain_reasoning']['active_domains']
            if len(active_domains) > 1:
                print(f"üè∑Ô∏è  Other Domains: {', '.join([d.title() for d in active_domains if d != overall['primary_domain']])}")
            
            print(f"üì§ Expected Output: {overall['expected_output'].replace('_', ' ').title()}")
            print(f"üìä Complexity: {overall['complexity_level'].title()}")
            
            # Enhanced coherence display
            coherence_icon = "‚úÖ" if overall['is_coherent'] else "‚ùå"
            absurdity = overall['absurdity_score']
            print(f"üß† Coherent: {coherence_icon} (Absurdity: {absurdity:.1%})")
            
            if not overall['is_coherent']:
                print(f"üö® Issues: {', '.join(context['coherence_check']['issues'])}")
            
            # Quality indicators
            quality = overall['context_quality']
            quality_emoji = "‚≠ê‚≠ê‚≠ê" if quality > 0.8 else "‚≠ê‚≠ê" if quality > 0.6 else "‚≠ê"
            print(f"{quality_emoji} Context Quality: {quality:.1%}")
            
            print(f"üéØ Selected Model: {model} ({confidence:.1%})")
            print(f"üí≠ Reasoning: {reasoning}")
            
            # Offer detailed analysis
            print(f"\nüîç Want ultra-detailed analysis? (y/n): ", end="")
            detail = input().strip().lower()
            
            if detail in ['y', 'yes']:
                print("\nüìã ULTRA-DETAILED ANALYSIS:")
                print("-" * 40)
                
                # Intent breakdown
                intent_scores = context['intent_analysis']['intent_scores']
                print(f"üéØ Intent Confidence Breakdown:")
                for intent_type, score in sorted(intent_scores.items(), key=lambda x: x[1], reverse=True):
                    if score > 0:
                        print(f"   ‚Ä¢ {intent_type.replace('_', ' ').title()}: {score:.2f}")
                
                # Domain evidence
                domain_evidence = context['domain_reasoning']['domain_evidence']
                print(f"\nüè∑Ô∏è  Domain Evidence Analysis:")
                for domain_type, evidence in domain_evidence.items():
                    if evidence['score'] > 0.1:
                        print(f"   ‚Ä¢ {domain_type.title()}: {evidence['score']:.2f}")
                        for item in evidence['evidence'][:2]:  # Show top 2 evidence items
                            print(f"     - {item}")
                
                # Complexity factors
                complexity_factors = context['task_complexity']['factors']
                print(f"\nüìä Complexity Factor Breakdown:")
                for factor, score in complexity_factors.items():
                    print(f"   ‚Ä¢ {factor.replace('_', ' ').title()}: {score:.2f}")
                
                # Quality factors
                quality_factors = overall['quality_factors']
                print(f"\n‚≠ê Quality Factor Analysis:")
                for factor, score in quality_factors.items():
                    print(f"   ‚Ä¢ {factor.replace('_', ' ').title()}: {score:.1%}")
                
                # Relationship analysis
                relationships = context['relationship_analysis']
                if relationships['relationships']:
                    print(f"\nüîó Detected Relationships:")
                    for rel in relationships['relationships'][:3]:  # Show top 3
                        print(f"   ‚Ä¢ {rel['entity']} {rel['relationship']} {rel['concept']}")
                
        except Exception as e:
            print(f"‚ùå Error in ultra analysis: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    interactive_ultra_test()
