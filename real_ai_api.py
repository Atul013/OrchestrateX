#!/usr/bin/env python3
"""
Real AI Model Integration - Working Backend
Calls actual OpenRouter API for real AI responses
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
import os
import json
from datetime import datetime

app = Flask(__name__)
CORS(app, origins="*")

# Load API keys from orche.env
def load_api_config():
    """Load API configuration from orche.env"""
    config = {}
    env_file = 'orche.env'
    
    if os.path.exists(env_file):
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if '=' in line and not line.startswith('#') and not line.startswith('â€”'):
                    key, value = line.split('=', 1)
                    config[key.strip()] = value.strip()
    
    # Extract model configurations
    models = {
        'Llama 4 Maverick': {
            'api_key': config.get('PROVIDER_LLAMA3_API_KEY', '').strip(),
            'model_id': config.get('PROVIDER_LLAMA3_MODEL', 'meta-llama/llama-4-maverick:free')
        },
        'GLM4.5': {
            'api_key': config.get('PROVIDER_GLM45_API_KEY', '').strip(),
            'model_id': config.get('PROVIDER_GLM45_MODEL', 'z-ai/glm-4.5-air:free')
        },
        'GPT-OSS': {
            'api_key': config.get('PROVIDER_GPTOSS_API_KEY', '').strip(),
            'model_id': config.get('PROVIDER_GPTOSS_MODEL', 'openai/gpt-oss-120b:free')
        },
        'MoonshotAI Kimi': {
            'api_key': config.get('PROVIDER_KIMI_API_KEY', '').strip(),
            'model_id': config.get('PROVIDER_KIMI_MODEL', 'moonshotai/kimi-k2:free')
        },
        'Qwen3': {
            'api_key': config.get('PROVIDER_QWEN3_API_KEY', '').strip(),
            'model_id': config.get('PROVIDER_QWEN3_MODEL', 'qwen/qwen3-coder:free')
        },
        'TNG DeepSeek': {
            'api_key': config.get('PROVIDER_FALCON_API_KEY', '').strip(),
            'model_id': config.get('PROVIDER_FALCON_MODEL', 'tngtech/deepseek-r1t2-chimera:free')
        }
    }
    
    return models

def call_openrouter_api(api_key, model_id, prompt, max_tokens=2000):
    """Call OpenRouter API for real AI responses"""
    try:
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
            'HTTP-Referer': 'http://localhost:5176',
            'X-Title': 'OrchestrateX'
        }
        
        data = {
            'model': model_id,
            'messages': [{'role': 'user', 'content': prompt}],
            'max_tokens': max_tokens,
            'temperature': 0.7
        }
        
        response = requests.post(
            'https://openrouter.ai/api/v1/chat/completions',
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            usage = result.get('usage', {})
            
            return {
                'success': True,
                'content': content,
                'tokens_used': usage.get('total_tokens', 0),
                'cost_usd': usage.get('total_tokens', 0) * 0.000001  # Rough estimate
            }
        else:
            return {
                'success': False,
                'error': f'API error {response.status_code}: {response.text}'
            }
            
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'ok', 'real_ai': True})

@app.route('/api/orchestration/process', methods=['POST', 'OPTIONS'])
def process_real_orchestration():
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = '*'
        response.headers['Access-Control-Allow-Methods'] = 'POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
        return response
    
    try:
        data = request.get_json()
        prompt = data.get('prompt', 'No prompt provided')
        
        print(f"[{datetime.now()}] Processing real AI orchestration for: {prompt[:50]}...")
        
        # Load model configurations
        models = load_api_config()
        
        # Primary model (Llama 4 Maverick)
        primary_model = 'Llama 4 Maverick'
        primary_config = models[primary_model]
        
        print(f"[{datetime.now()}] Calling {primary_model}...")
        primary_result = call_openrouter_api(
            primary_config['api_key'],
            primary_config['model_id'],
            prompt
        )
        
        if not primary_result['success']:
            raise Exception(f"Primary model failed: {primary_result['error']}")
        
        # Get critiques from other models
        critiques = []
        critique_models = ['GLM4.5', 'GPT-OSS']  # Limit to 2 for speed
        
        for model_name in critique_models:
            if model_name in models:
                model_config = models[model_name]
                critique_prompt = f"Provide a brief constructive critique of this response to '{prompt[:100]}': {primary_result['content'][:500]}"
                
                print(f"[{datetime.now()}] Getting critique from {model_name}...")
                critique_result = call_openrouter_api(
                    model_config['api_key'],
                    model_config['model_id'],
                    critique_prompt,
                    max_tokens=500
                )
                
                if critique_result['success']:
                    critiques.append({
                        'model_name': model_name,
                        'critique_text': critique_result['content'],
                        'tokens_used': critique_result['tokens_used'],
                        'cost_usd': critique_result['cost_usd'],
                        'latency_ms': 2000,
                        'success': True
                    })
        
        # Format response
        response_data = {
            'success': True,
            'primary_response': {
                'success': True,
                'model_name': primary_model,
                'response_text': primary_result['content'],
                'tokens_used': primary_result['tokens_used'],
                'cost_usd': primary_result['cost_usd'],
                'latency_ms': 3000,
                'confidence_score': 0.95
            },
            'critiques': critiques,
            'total_cost': primary_result['cost_usd'] + sum(c['cost_usd'] for c in critiques),
            'api_calls': 1 + len(critiques),
            'success_rate': 100.0,
            'timestamp': datetime.now().isoformat(),
            'selected_model': primary_model,
            'refinement_available': len(critiques) > 0
        }
        
        print(f"[{datetime.now()}] Real AI orchestration completed!")
        return jsonify(response_data)
        
    except Exception as e:
        print(f"[{datetime.now()}] Error in real orchestration: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    print("=== REAL AI MODEL BACKEND ===")
    print("Starting real OpenRouter API integration...")
    print("Server: http://localhost:8002")
    app.run(host='127.0.0.1', port=8002, debug=False, threaded=True)
