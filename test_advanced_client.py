#!/usr/bin/env python3
"""
Simple test for the Advanced OrchestrateX Client
Tests the complete multi-model orchestration with critiques
"""

import asyncio
import logging
import os
from advanced_client import MultiModelOrchestrator, format_for_ui

# Set up logging to be less verbose for demo
logging.basicConfig(level=logging.WARNING, format='%(levelname)s - %(message)s')

async def test_advanced_client():
    """Test the advanced multi-model orchestration client"""
    
    print("ğŸš€ Testing Advanced OrchestrateX Client")
    print("=" * 45)
    
    # Check for API keys
    has_openrouter = bool(os.getenv("OPENROUTER_API_KEY"))
    print(f"OpenRouter API Key: {'âœ… Found' if has_openrouter else 'âŒ Not found (will use simulation)'}")
    
    async with MultiModelOrchestrator() as orchestrator:
        
        # Test prompt
        prompt = "Write a Python function to implement a secure user authentication system"
        
        print(f"\nğŸ¯ Test Prompt: {prompt}")
        print(f"ğŸ“¡ Starting multi-model orchestration...")
        
        # Get complete orchestration with critiques
        result = await orchestrator.orchestrate_with_critiques(prompt)
        
        print(f"\nğŸ“Š Orchestration Results:")
        print(f"Overall Success: {'âœ…' if result.success else 'âŒ'}")
        print(f"Selected Model: {result.selected_model}")
        print(f"Total Time: {result.total_latency_ms}ms")
        print(f"Total Cost: ${result.total_cost_usd:.4f}")
        
        # Primary Response
        print(f"\nğŸ¯ Primary Response ({result.primary_response.model_name}):")
        print(f"Success: {'âœ…' if result.primary_response.success else 'âŒ'}")
        print(f"Latency: {result.primary_response.latency_ms}ms")
        print(f"Cost: ${result.primary_response.cost_usd:.4f}")
        if result.primary_response.success:
            preview = result.primary_response.response_text[:150]
            print(f"Preview: {preview}...")
        else:
            print(f"Error: {result.primary_response.error_message}")
        
        # Critique Responses
        print(f"\nğŸ“ Critique Responses ({len(result.critique_responses)} models):")
        successful_critiques = 0
        
        for i, critique in enumerate(result.critique_responses, 1):
            status = "âœ…" if critique.success else "âŒ"
            print(f"{i}. {status} {critique.model_name}")
            print(f"   Latency: {critique.latency_ms}ms | Cost: ${critique.cost_usd:.4f}")
            
            if critique.success:
                successful_critiques += 1
                preview = critique.response_text[:100]
                print(f"   Preview: {preview}...")
            else:
                print(f"   Error: {critique.error_message}")
        
        print(f"\nğŸ“ˆ Summary:")
        print(f"Successful Critiques: {successful_critiques}/{len(result.critique_responses)}")
        
        # Test UI formatting
        print(f"\nğŸ–¥ï¸ UI Integration Test:")
        ui_data = format_for_ui(result)
        print(f"UI Data Keys: {list(ui_data.keys())}")
        print(f"Primary Model: {ui_data['primary']['model']}")
        print(f"Critique Models: {[c['model'] for c in ui_data['critiques']]}")
        
        # Show confidence scores
        print(f"\nğŸ¯ Model Confidence Scores:")
        for model, score in result.model_confidence_scores.items():
            marker = "ğŸ‘‘" if model == result.selected_model else "  "
            print(f"{marker} {model}: {score:.3f}")
        
        # Statistics
        print(f"\nğŸ“Š Client Statistics:")
        orchestrator.print_stats()
        
        return result

async def test_concurrent_performance():
    """Test concurrent performance with multiple prompts"""
    
    print(f"\nâš¡ Concurrent Performance Test")
    print("=" * 35)
    
    test_prompts = [
        "Implement a binary search algorithm",
        "Explain machine learning basics",
        "Design a REST API architecture"
    ]
    
    async with MultiModelOrchestrator() as orchestrator:
        
        print(f"Testing {len(test_prompts)} prompts concurrently...")
        
        # Run multiple orchestrations concurrently
        tasks = [
            orchestrator.orchestrate_with_critiques(prompt) 
            for prompt in test_prompts
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        print(f"\nğŸ“Š Concurrent Results:")
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                print(f"{i}. âŒ Failed: {result}")
            else:
                print(f"{i}. {'âœ…' if result.success else 'âŒ'} {result.selected_model} - {result.total_latency_ms}ms")
        
        orchestrator.print_stats()

if __name__ == "__main__":
    async def main():
        try:
            # Test basic functionality
            result = await test_advanced_client()
            
            # Test concurrent performance
            await test_concurrent_performance()
            
            print(f"\nğŸ‰ All tests completed!")
            
        except Exception as e:
            print(f"\nâŒ Test failed: {e}")
            import traceback
            traceback.print_exc()
    
    asyncio.run(main())
