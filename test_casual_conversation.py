#!/usr/bin/env python3
"""    for prompt in casual_prompts:
        # Analyze context
        context = analyzer.analyze_ultra_context(prompt)
        # Select model
        model, confidence, reasoning = analyzer.select_ultra_model(prompt)
        
        print(f'ğŸ“ Prompt: "{prompt}"')
        print(f'   ğŸ¤– Model: {model} ({confidence:.1f}%)')
        print(f'   ğŸ¯ Intent: {context["intent"]}')
        print(f'   ğŸ·ï¸  Domain: {context["primary_domain"]}')
        print(f'   ğŸ“¤ Output: {context["expected_output"]}')
        print(f'   â­ Quality: {context["quality_score"]:.1f}%')
        print(f'   ğŸ’­ Why: {reasoning[:80]}...')
        print()est for casual conversation handling
"""

from ultra_context_analyzer import UltraContextAnalyzer

def test_casual_prompts():
    analyzer = UltraContextAnalyzer()
    
    # Test casual conversation prompts
    casual_prompts = [
        'hey how are ya',
        'hello',
        'hi there', 
        'sup',
        'good morning',
        'thanks',
        'bye',
        'see you later',
        'whats up',
        'how ya doing'
    ]
    
    print('ğŸ§ª Testing Casual Conversation Handling:')
    print('=' * 60)
    
    for prompt in casual_prompts:
        # Analyze context
        context = analyzer.analyze_ultra_context(prompt)
        # Select model
        model, confidence, reasoning = analyzer.select_ultra_model(prompt)
        
        print(f'ğŸ“ Prompt: "{prompt}"')
        print(f'   ğŸ¤– Model: {model} ({confidence:.1f}%)')
        print(f'   ğŸ¯ Intent: {context["overall_context"]["primary_intent"]}')
        print(f'   ğŸ·ï¸  Domain: {context["overall_context"]["primary_domain"]}')
        print(f'   ğŸ“¤ Output: {context["overall_context"]["expected_output"]}')
        print(f'   â­ Quality: {context["overall_context"]["context_quality"]:.1f}%')
        print(f'   ğŸ’­ Why: {reasoning[:80]}...')
        print()

if __name__ == "__main__":
    test_casual_prompts()
